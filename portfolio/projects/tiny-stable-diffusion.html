<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tiny-stable-diffusion | Junyeong Song</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="projects.css">
</head>

<body>
    <a href="../index.html" class="back-btn" data-i18n="nav.back">‚Üê Back to Portfolio</a>
    <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
        <span class="sun-icon">‚òÄÔ∏è</span>
        <span class="moon-icon">üåô</span>
    </button>
    <button class="lang-toggle" id="langToggle" aria-label="Toggle language">Ìïú</button>

    <main class="container project-detail">
        <div class="project-header">
            <span class="project-icon-large">üñºÔ∏è</span>
            <div class="project-meta">
                <h1>tiny-stable-diffusion</h1>
                <span class="project-date">2024.12 - Present</span>
            </div>
        </div>

        <section class="project-section">
            <h2 data-i18n="proj.overview">üìã Project Overview</h2>
            <p data-i18n="sd.overview">A lightweight, educational implementation of Stable Diffusion 3 (SD3) built from
                scratch in PyTorch.</p>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.problem">üéØ Problem Definition & Goals</h2>
            <ul class="feature-list">
                <li data-i18n="sd.problem"><strong>Problem:</strong> State-of-the-art Stable Diffusion models are too
                    large to train on consumer hardware.</li>
                <li data-i18n="sd.goal1"><strong>Goal 1:</strong> Create a scaled-down SD3 implementation trainable on a
                    single consumer GPU.</li>
                <li data-i18n="sd.goal2"><strong>Goal 2:</strong> Implement the complete pipeline including VAE, text
                    encoders, and MMDiT from scratch.</li>
                <li data-i18n="sd.goal3"><strong>Goal 3:</strong> Extend to video/GIF generation using motion modules.
                </li>
            </ul>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.features">‚öôÔ∏è Key Features & Contributions</h2>
            <ul class="feature-list">
                <li data-i18n="sd.feat1"><strong>MMDiT Architecture:</strong> Implemented Multi-Modal Diffusion
                    Transformer.</li>
                <li data-i18n="sd.feat2"><strong>Rectified Flow:</strong> Used modern flow-based formulation for
                    straighter sampling.</li>
                <li data-i18n="sd.feat3"><strong>Complete Pipeline:</strong> Built VAE, CLIP/T5 text conditioning, and
                    full denoising pipeline.</li>
                <li data-i18n="sd.feat4"><strong>Motion Module:</strong> Added temporal attention layers for animated
                    GIFs.</li>
                <li data-i18n="sd.feat5"><strong>Educational Design:</strong> Clean, well-commented codebase.</li>
            </ul>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.challenges">üîß Technical Challenges & Solutions</h2>
            <ul class="feature-list">
                <li data-i18n="sd.chal1"><strong>Memory Constraints:</strong> Reduced model size while preserving
                    innovations.</li>
                <li data-i18n="sd.chal2"><strong>VAE Training Stability:</strong> Implemented KL annealing and
                    perceptual loss.</li>
                <li data-i18n="sd.chal3"><strong>Text-Image Alignment:</strong> Used CFG and improved cross-attention.
                </li>
                <li data-i18n="sd.chal4"><strong>Motion Consistency:</strong> Added temporal attention for smooth
                    transitions.</li>
            </ul>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.results">üìà Results & Learnings</h2>
            <ul class="feature-list">
                <li data-i18n="sd.result1"><strong>Successful Generation:</strong> Model generates coherent images from
                    text prompts.</li>
                <li data-i18n="sd.result2"><strong>Training Efficiency:</strong> Entire model trainable on single RTX
                    3090.</li>
                <li data-i18n="sd.result3"><strong>Educational Impact:</strong> Codebase serves as learning reference.
                </li>
                <li data-i18n="sd.result4"><strong>Key Learning:</strong> Gained comprehensive understanding of SD3
                    architecture.</li>
            </ul>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.tech">üõ†Ô∏è Technologies</h2>
            <div class="tech-tags">
                <span class="tech-tag">PyTorch</span>
                <span class="tech-tag">Stable Diffusion 3</span>
                <span class="tech-tag">MMDiT</span>
                <span class="tech-tag">Rectified Flow</span>
                <span class="tech-tag">VAE</span>
                <span class="tech-tag">CLIP</span>
                <span class="tech-tag">T5</span>
            </div>
        </section>

        <section class="project-section">
            <h2 data-i18n="proj.links">üîó Links</h2>
            <div class="project-links">
                <a href="https://github.com/junyeong-nero/tiny-stable-diffusion" target="_blank" class="project-link">
                    üíª GitHub Repository
                </a>
            </div>
        </section>
    </main>

    <script src="../theme.js"></script>
    <script src="../lang.js"></script>
</body>

</html>